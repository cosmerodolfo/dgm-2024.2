{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "from satellite2map.datasets import Maps\n",
    "from satellite2map.models.generator import UnetGenerator\n",
    "from satellite2map.models.discriminator import ConditionalDiscriminator\n",
    "from satellite2map.losses import GeneratorLoss, DiscriminatorLoss\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Pix implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class hyperparameters:\n",
    "    # training hyperparams\n",
    "    n_epochs: int = 32\n",
    "    batch_size: int = 32\n",
    "    lr: float = 2e-3\n",
    "\n",
    "hyperparams = hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": hyperparams.lr,\n",
    "    \"architecture\": \"Unet\",\n",
    "    \"dataset\": \"Maps\",\n",
    "    \"epochs\": hyperparams.n_epochs,\n",
    "    \"batch_size\": hyperparams.batch_size,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms=v2.Compose([\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[127.5, 127.5, 127.5], std=[127.5, 127.5, 127.5]),\n",
    "])\n",
    "\n",
    "train_dataset = Maps('../data/raw/maps_ny/train', transforms=transforms)\n",
    "val_dataset   = Maps('../data/raw/maps_ny/val', transforms=transforms)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=hyperparams.batch_size, shuffle=True)\n",
    "val_dataloader   = DataLoader(val_dataset,   batch_size=hyperparams.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = UnetGenerator().to(DEVICE)\n",
    "discriminator = ConditionalDiscriminator(hyperparams.num_classes).to(DEVICE)\n",
    "\n",
    "g_criterion = GeneratorLoss()\n",
    "d_criterion = DiscriminatorLoss()\n",
    "\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=hyperparams.lr, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=hyperparams.lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(hyperparams.n_epochs):\n",
    "    ge_loss=0.\n",
    "    de_loss=0.\n",
    "    start = time.time()\n",
    "    for x, real in tqdm(train_dataloader):\n",
    "        x = x.to(DEVICE)\n",
    "        real = real.to(DEVICE)\n",
    "\n",
    "        # Generator`s loss\n",
    "        fake = generator(x)\n",
    "        fake_pred = discriminator(fake, x)\n",
    "        g_loss = g_criterion(fake, real, fake_pred)\n",
    "\n",
    "        # Discriminator`s loss\n",
    "        fake = generator(x).detach()\n",
    "        fake_pred = discriminator(fake, x)\n",
    "        real_pred = discriminator(real, x)\n",
    "        d_loss = d_criterion(fake_pred, real_pred)\n",
    "\n",
    "        # Generator`s params update\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Discriminator`s params update\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        # add batch losses\n",
    "        ge_loss += g_loss.item()\n",
    "        de_loss += d_loss.item()\n",
    "\n",
    "    # obttain per epoch losses\n",
    "    g_loss = ge_loss/len(train_dataloader)\n",
    "    d_loss = de_loss/len(train_dataloader)\n",
    "    # count timeframe\n",
    "    end = time.time()\n",
    "    tm = (end - start)\n",
    "    wandb.log({\"generator_loss\": g_loss, \"discriminator_loss\": d_loss, \"time\": tm})\n",
    "    torch.save(generator.state_dict(), f'./models/pix2pix/checkpoints/generator')\n",
    "    torch.save(discriminator.state_dict(), f'./models/pix2pix/checkpoints/generator')\n",
    "    print(f\"[Epoch {epoch+1}/{hyperparams.epochs}] [G loss: {g_loss}] [D loss: {d_loss}] ETA: {tm}\")\n",
    "logger.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia376",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
